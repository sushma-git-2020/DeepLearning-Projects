{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "5-HW_IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y-KYw-a40uQ",
        "colab_type": "text"
      },
      "source": [
        "### IMDB Dataset-Keras\n",
        "\n",
        "( https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data)\n",
        "\n",
        "This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gF4F9t340uV",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Load the dataset and required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCHXXB-640ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMg7iouF6sMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the dataset but only keep the top 5000 words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htCD-mzK40ur",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** truncate and pad input sequences for 500 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUKchzDZTGYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length=500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_EcG5f540uw",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Create the model. Use embedding and set embedding vector size to 32. Create a simple LSTM layer with 100 nodes. Compile the model and test its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "InBmUQ_7UHa5",
        "outputId": "a27ea1cc-efac-43c7-fba4-8783149ad2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "embedding_vecor_length=32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, output_dim=32, input_length = max_length))\n",
        "model.add(LSTM(100, return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_fmffyiDY8P",
        "colab_type": "code",
        "outputId": "daac8f99-4f9a-4a5a-fe16-bc2fe9e7090e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32, verbose=2)\n",
        "\n",
        "scores=model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            " - 383s - loss: 0.5459 - accuracy: 0.7175 - val_loss: 0.6402 - val_accuracy: 0.6106\n",
            "Epoch 2/10\n",
            " - 400s - loss: 0.4297 - accuracy: 0.7997 - val_loss: 0.3253 - val_accuracy: 0.8692\n",
            "Epoch 3/10\n",
            " - 399s - loss: 0.3023 - accuracy: 0.8771 - val_loss: 0.3170 - val_accuracy: 0.8686\n",
            "Epoch 4/10\n",
            " - 399s - loss: 0.2419 - accuracy: 0.9045 - val_loss: 0.2937 - val_accuracy: 0.8798\n",
            "Epoch 5/10\n",
            " - 401s - loss: 0.2088 - accuracy: 0.9187 - val_loss: 0.3114 - val_accuracy: 0.8765\n",
            "Epoch 6/10\n",
            " - 400s - loss: 0.1834 - accuracy: 0.9287 - val_loss: 0.3133 - val_accuracy: 0.8747\n",
            "Epoch 7/10\n",
            " - 397s - loss: 0.1628 - accuracy: 0.9395 - val_loss: 0.3197 - val_accuracy: 0.8673\n",
            "Epoch 8/10\n",
            " - 396s - loss: 0.1472 - accuracy: 0.9430 - val_loss: 0.3384 - val_accuracy: 0.8693\n",
            "Epoch 9/10\n",
            " - 396s - loss: 0.1229 - accuracy: 0.9538 - val_loss: 0.4037 - val_accuracy: 0.8612\n",
            "Epoch 10/10\n",
            " - 410s - loss: 0.1045 - accuracy: 0.9610 - val_loss: 0.4059 - val_accuracy: 0.8690\n",
            "Accuracy: 86.90%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}